misc:
  # True: train 'networkName' NN with 'train_dataset' in 'data_save_root'
  # False: use 'pretrain_file' NN to test data from 'inference_data_dir' in 'data_save_root'
  training: True

  data_save_root: ./data
  run_save_root: ./runs
  run_name: dtepr_mhsa

  # dataset name for training, shall be available in 'data_save_root'
  train_dataset: dtepr
  ## only used for inference
  # dir containing all datasets for inference, shall be available in 'data_save_root'
  inference_data_dir: inference
  # dir containing the trained nn weights, shall be available in 'run_save_root'
  pretrain_dir: dtepr_mhsa_demo

  # 
  if_embed_time: True

  verbose: True
  debug: False
  batch_size: 256
  print_step: 10
  num_workers: 0
  

embedding:
  base_emb_size: 64

model:
  networkName: mhsa
  fc_dropout: 0.1

  # only for mhsa
  num_encoder_layers: 4
  nhead: 8
  dim_feedforward: 256
  dropout: 0.1
  
  # only for LSTM
  # whether include self-attention layer
  attention: False
  # LSTM or GRU
  rnn_type: LSTM
  hidden_size: 128

optimiser:
  # Adam or SGD
  optimizer: Adam
  max_epoch: 100
  lr: 0.001
  weight_decay: 0.000001
  # for Adam
  beta1: 0.9
  beta2: 0.999
  # for SGD
  momentum: 0.98

  # for warmup
  num_warmup_epochs: 2
  num_training_epochs: 50
  # for decay
  patience: 3
  lr_step_size: 1
  lr_gamma: 0.1